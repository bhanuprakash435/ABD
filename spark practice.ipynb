{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('sample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating rdd's\n",
    "rdd = spark.sparkContext.parallelize(['ghana','spoorthi','manoj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghana', 'spoorthi', 'manoj']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd.filter(lambda x:x.startswith('s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spoorthi']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd3 = spark.sparkContext.textFile(r'C:\\Users\\User\\Desktop\\sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " '',\n",
       " 'flip',\n",
       " '',\n",
       " 'f the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.One of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd3.filter(lambda x:x.startswith('f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = rdd3.flatMap(lambda word:word.split(' '))\n",
    "value = word.map(lambda word:(word,1))\n",
    "count = value.reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('One', 1),\n",
       " ('', 2),\n",
       " ('common', 10),\n",
       " ('formats', 10),\n",
       " ('is', 20),\n",
       " ('methods,', 10),\n",
       " ('results,', 10),\n",
       " ('genre,', 10),\n",
       " ('mirrors', 10),\n",
       " ('traditional', 10),\n",
       " ('publication', 10),\n",
       " ('of', 29),\n",
       " ('scientific', 10),\n",
       " ('research', 10),\n",
       " ('discipline.', 10),\n",
       " ('are', 20),\n",
       " ('this', 10),\n",
       " ('pattern', 10),\n",
       " ('may', 10),\n",
       " ('use', 10),\n",
       " ('alternative', 10),\n",
       " ('as', 10),\n",
       " ('format,', 10),\n",
       " ('an', 10),\n",
       " ('details', 10),\n",
       " ('must', 10),\n",
       " ('fix', 10),\n",
       " ('quality', 10),\n",
       " ('writing', 10),\n",
       " ('useful', 10),\n",
       " ('report.', 10),\n",
       " ('numbers', 10),\n",
       " ('in', 10),\n",
       " ('financial', 10),\n",
       " ('disastrous', 10),\n",
       " ('consequences.', 1),\n",
       " ('flip', 1),\n",
       " ('f', 1),\n",
       " ('the', 60),\n",
       " ('most', 10),\n",
       " ('for', 20),\n",
       " ('presenting', 10),\n",
       " ('reports', 10),\n",
       " ('IMRAD—introduction,', 10),\n",
       " ('and', 60),\n",
       " ('discussion.', 10),\n",
       " ('This', 10),\n",
       " ('structure,', 10),\n",
       " ('standard', 10),\n",
       " ('summons', 10),\n",
       " ('ethos', 10),\n",
       " ('credibility', 10),\n",
       " ('that', 10),\n",
       " ('Reports', 10),\n",
       " ('not', 10),\n",
       " ('required', 10),\n",
       " ('to', 40),\n",
       " ('follow', 10),\n",
       " ('methods', 10),\n",
       " ('such', 10),\n",
       " ('problem-solution', 10),\n",
       " ('wherein', 10),\n",
       " ('author', 10),\n",
       " ('first', 10),\n",
       " ('lists', 10),\n",
       " ('issue', 10),\n",
       " ('then', 10),\n",
       " ('what', 10),\n",
       " ('be', 10),\n",
       " ('done', 10),\n",
       " ('problem.', 10),\n",
       " ('Transparency', 10),\n",
       " ('a', 30),\n",
       " ('focus', 10),\n",
       " ('on', 10),\n",
       " ('keys', 10),\n",
       " ('Accuracy', 10),\n",
       " ('also', 10),\n",
       " ('important.', 10),\n",
       " ('Faulty', 10),\n",
       " ('report', 10),\n",
       " ('could', 10),\n",
       " ('lead', 10),\n",
       " ('consequences.One', 9)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark dataframes\n",
    "columns = ['currency','rate']\n",
    "inputdata = [('Euro', 90), ('Pound', 100), ('Yuan', 11), ('Yen', 2), ('US Dollar', 74), ('K Dinar', 242)]\n",
    "\n",
    "\n",
    "curr = spark.sparkContext.parallelize(inputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = curr.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|     Euro| 90|\n",
      "|    Pound|100|\n",
      "|     Yuan| 11|\n",
      "|      Yen|  2|\n",
      "|US Dollar| 74|\n",
      "|  K Dinar|242|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_1','Currency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "| Currency| _2|\n",
      "+---------+---+\n",
      "|     Euro| 90|\n",
      "|    Pound|100|\n",
      "|     Yuan| 11|\n",
      "|      Yen|  2|\n",
      "|US Dollar| 74|\n",
      "|  K Dinar|242|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(curr).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "| currency|rate|\n",
      "+---------+----+\n",
      "|     Euro|  90|\n",
      "|    Pound| 100|\n",
      "|     Yuan|  11|\n",
      "|      Yen|   2|\n",
      "|US Dollar|  74|\n",
      "|  K Dinar| 242|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/User/Desktop/sample already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3e1b9b7d390d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\User\\Desktop\\sample'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/User/Desktop/sample already exists.;"
     ]
    }
   ],
   "source": [
    "df.write.format('csv').save(r'C:\\Users\\User\\Desktop\\sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/User/Desktop/sample already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-7d30ad84a0f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\User\\Desktop\\sample'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/User/Desktop/sample already exists.;"
     ]
    }
   ],
   "source": [
    "df.repartition(1).write.format('csv').save(r'C:\\Users\\User\\Desktop\\sample')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    df.curr.map(lambda x: x[0]+\",\"+str(x[1])).rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'curr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-95884bd0c100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\User\\Desktop\\s1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\BigData\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1398\u001b[0m         \"\"\"\n\u001b[0;32m   1399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1401\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'curr'"
     ]
    }
   ],
   "source": [
    "df.curr.map(lambda x:x[0]+\",\"+str(x[1])).repartition(1).saveTextFile(r'C:\\Users\\User\\Desktop\\s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
